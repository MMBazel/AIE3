{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!\n",
        "\n",
        "We'll be doing the following in today's notebook:\n",
        "\n",
        "1. Task 1: Simple Assistant\n",
        "2. Task 2: Adding Tools\n",
        "  - Task 2a: Creating an Assistant with File Search Tool\n",
        "  - Task 2b: Creating an Assistant with Code Interpreter Tool\n",
        "  - Task 2c: Creating an Assistant with Function Calling Tool\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Colab Specific Instructions:\n",
        "\n",
        "To get started, please make a copy of this notebook using `File > Save a copy in Drive`\n",
        "\n",
        "![image](https://i.imgur.com/rNzMEfs.png)\n",
        "\n",
        "You will be expected to submit a GitHub link to the completed notebook, so if you're completing the assignment on Colab - you'll want to download the notebook when you have completed it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePayyL6at6LS",
        "outputId": "4fb742b3-c650-44c2-8c58-2d83c81a766b"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "9c567820-9633-44fd-dad6-e7dd1ca46d97"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Task 1: Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### ðŸ—ï¸ Build Activity ðŸ—ï¸\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"Lola\" # @param {type: \"string\"}\n",
        "instructions = \"You're a funny, charming, colorful assistant. \" # @param {type: \"string\"}\n",
        "model = \"gpt-4o\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistant\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "86ecb682-a04d-4687-94d3-18a4b99ffc07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_oJrTUoQL3oNfgj1tfxKG6yRn', created_at=1718139821, description=None, instructions=\"You're a funny, charming, colorful assistant. \", metadata={}, model='gpt-4o', name='Lola', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "6ba2a680-a595-40e8-fa5d-5784b9614b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_VSMYW8ESCVJzoUachtD0jMem', created_at=1718139821, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"How many Legos would it take to build the White House?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "dfde63a0-3b6f-4f01-b76c-ec84baefe5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_ZhT11KXcTfgcxpYfW0ISryDd', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='How many Legos would it take to build the White House?'), type='text')], created_at=1718139821, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_VSMYW8ESCVJzoUachtD0jMem')"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### ðŸ—ï¸ Build Activity ðŸ—ï¸\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = \"Please state the assumptions, state the methodology, and what questions would need to be answered. Then walk through the problem step-by and provide a numeric range at the end.\" # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "56aa670d-8f22-4ef4-b5f4-e568c6f7cefb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Run(id='run_vFZUAbkiN4IOUQ1Tsh6rIJzn', assistant_id='asst_oJrTUoQL3oNfgj1tfxKG6yRn', cancelled_at=None, completed_at=None, created_at=1718139821, expires_at=1718140421, failed_at=None, incomplete_details=None, instructions='Please state the assumptions, state the methodology, and what questions would need to be answered. Then walk through the problem step-by and provide a numeric range at the end.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_VSMYW8ESCVJzoUachtD0jMem', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "89f3df27-d5a3-4095-9422-6271db52cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n"
          ]
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "3916ed59-7eba-471f-9eb7-d8d2f4658012"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_FfK51D8mJzPS4HtzscTe19NB', assistant_id='asst_oJrTUoQL3oNfgj1tfxKG6yRn', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"To estimate the number of LEGO bricks required to build a model of the White House, we need to make several assumptions and outline a methodology. Then, we can calculate a numeric range for the number of pieces.\\n\\n### Assumptions:\\n1. **Type of LEGO Bricks**: We'll assume a mixture of standard 2x4 and 1x2 LEGO bricks for simplicity.\\n2. **Scale of the Model**: We'll use a specific scale to reduce the size of the LEGO model compared to the actual White House (e.g., 1:50 or 1:100).\\n3. **Dimension Clarity**: We'll focus on the exterior dimensions of the White House main building.\\n4. **Detail Level**: We assume a medium level of detailâ€”enough to be recognizable but not replicating every minute feature.\\n5. **Windows and Doors**: We'll assume some bricks will be substituted with transparent or specialized pieces for windows and doors but will approximate with standard bricks.\\n\\n### Methodology:\\n1. **Determine the Scale**: Choose an appropriate scale for the LEGO model.\\n2. **Calculate the Actual Dimensions**: Gather the dimensions of the actual White House, focusing on the main building.\\n3. **Convert Dimensions to LEGO Units**: Using the chosen scale, convert real-world dimensions to LEGO brick dimensions.\\n4. **Estimate Wall Area**: Calculate the exterior wall area of the White House.\\n5. **Calculate Brick Count per Area**: Determine how many LEGO bricks fit per given wall area (ignoring interior builds for simplicity).\\n6. **Sum Total Bricks**: Estimate the total number of bricks by multiplying the number of bricks per unit area by the total exterior wall area.\\n\\n### Questions to be Answered:\\n1. What are the actual dimensions of the White House's main building (height, width, depth)?\\n2. What scale will be used for the LEGO model?\\n3. How many LEGO bricks fit into an area corresponding to the scaled dimensions of the building?\\n4. What level of detail, and thus the density of blocks and specialized pieces, is appropriate for the model?\\n\\n### Step-by-Step Calculation:\\n\\n#### 1. Determine the Scale\\nAssume a 1:100 scale.\\n\\n#### 2. Actual Dimensions\\n- **Height**: ~21 meters (approximately 70 feet)\\n- **Width**: ~51 meters (approximately 170 feet) including porticos\\n- **Depth**: ~26 meters (approximately 85 feet)\\n\\n#### 3. Convert Dimensions to LEGO Units\\n1 LEGO brick's height (1 standard unit) corresponds roughly to 1.2 cm.\\n1 LEGO brick's length (2x4) corresponds roughly to 1.6 cm per stud.\\n\\nUsing 1:100 scale:\\n- Scaled Height: 21m / 100 = 0.21m or 21 cm â†’ ~18 bricks tall (21 cm / 1.2 cm per brick)\\n- Scaled Width: 51m / 100 = 0.51m or 51 cm â†’ ~32 studs wide (51 cm / 1.6 cm per stud)\\n- Scaled Depth: 26m / 100 = 0.26m or 26 cm â†’ ~16 studs deep (26 cm / 1.6 cm per stud)\\n\\n#### 4. Estimate Wall Area\\n- Total Height Bricks: 18 bricks\\n- Total Width Studs: 32 studs\\n- Total Depth Studs: 16 studs\\n\\nFront and Back Walls: \\n- Height x Width x 2 = (18 bricks * 32 bricks * 2) = 1,152 square-brick units\\nSide Walls:\\n- Height x Depth x 2 = (18 bricks * 16 bricks * 2) = 576 square-brick units\\nTotal Wall area = 1,152 + 576 = 1,728 square-brick units\\n\\n#### 5. Calculate Brick Count per Area\\nAssuming that each wall area unit (1 brick unit) requires roughly 1 standard brick:\\n\\nTotal Bricks Needed:\\n- Without detailed calculation adjustment: 1,728\\n- Assuming half area can be windows/doors (with more specialized brick size fraction):\\n  - Roughly 1,000 to 1,500 bricks for a detailed model\\n  - Adding interior/exterior detailed design increase range to roughly 2,000 to 3,000 bricks\\n\\n### Numeric Range:\\nPotentially between 2,000 to 3,000 LEGO bricks to build a moderately detailed 1:100 scale model of the White House.\"), type='text')], created_at=1718139822, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_vFZUAbkiN4IOUQ1Tsh6rIJzn', status=None, thread_id='thread_VSMYW8ESCVJzoUachtD0jMem')"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9_V_XlaVH7"
      },
      "source": [
        "### Streaming Our Runs\n",
        "\n",
        "With recent upgrades to the Assistant API - we can now *stream* our outputs!\n",
        "\n",
        "In order to do this - we'll need something called an `EventHandler` which will help us to decide on what actions to take based on the output of the LLM.\n",
        "\n",
        "Let's build it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "YvH-FUbQawcN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBieRYvbAmt"
      },
      "source": [
        "Now we can create our `run` and stream the output as it comes in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ax_0ZfbF_I",
        "outputId": "1bbddea1-a145-4a0d-b662-73f7703a511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > To estimate the number of LEGO bricks required to build a model of the White House, we need to make several assumptions and outline a methodology. Then, we can calculate a numeric range for the number of pieces.\n",
            "\n",
            "### Assumptions:\n",
            "1. **Type of LEGO Bricks**: We'll assume a mixture of standard 2x4 and 1x2 LEGO bricks for simplicity.\n",
            "2. **Scale of the Model**: We'll use a specific scale to reduce the size of the LEGO model compared to the actual White House (e.g., 1:50 or 1:100).\n",
            "3. **Dimension Clarity**: We'll focus on the exterior dimensions of the White House main building.\n",
            "4. **Detail Level**: We assume a medium level of detailâ€”enough to be recognizable but not replicating every minute feature.\n",
            "5. **Windows and Doors**: We'll assume some bricks will be substituted with transparent or specialized pieces for windows and doors but will approximate with standard bricks.\n",
            "\n",
            "### Methodology:\n",
            "1. **Determine the Scale**: Choose an appropriate scale for the LEGO model.\n",
            "2. **Calculate the Actual Dimensions**: Gather the dimensions of the actual White House, focusing on the main building.\n",
            "3. **Convert Dimensions to LEGO Units**: Using the chosen scale, convert real-world dimensions to LEGO brick dimensions.\n",
            "4. **Estimate Wall Area**: Calculate the exterior wall area of the White House.\n",
            "5. **Calculate Brick Count per Area**: Determine how many LEGO bricks fit per given wall area (ignoring interior builds for simplicity).\n",
            "6. **Sum Total Bricks**: Estimate the total number of bricks by multiplying the number of bricks per unit area by the total exterior wall area.\n",
            "\n",
            "### Questions to be Answered:\n",
            "1. What are the actual dimensions of the White House's main building (height, width, depth)?\n",
            "2. What scale will be used for the LEGO model?\n",
            "3. How many LEGO bricks fit into an area corresponding to the scaled dimensions of the building?\n",
            "4. What level of detail, and thus the density of blocks and specialized pieces, is appropriate for the model?\n",
            "\n",
            "### Step-by-Step Calculation:\n",
            "\n",
            "#### 1. Determine the Scale\n",
            "Assume a 1:100 scale.\n",
            "\n",
            "#### 2. Actual Dimensions\n",
            "- **Height**: ~21 meters (approximately 70 feet)\n",
            "- **Width**: ~51 meters (approximately 170 feet) including porticos\n",
            "- **Depth**: ~26 meters (approximately 85 feet)\n",
            "\n",
            "#### 3. Convert Dimensions to LEGO Units\n",
            "1 LEGO brick's height (1 standard unit) corresponds roughly to 1.2 cm.\n",
            "1 LEGO brick's length (2x4) corresponds roughly to 1.6 cm per stud.\n",
            "\n",
            "Using 1:100 scale:\n",
            "- Scaled Height: 21m / 100 = 0.21m or 21 cm â†’ ~18 bricks tall (21 cm / 1.2 cm per brick)\n",
            "- Scaled Width: 51m / 100 = 0.51m or 51 cm â†’ ~32 studs wide (51 cm / 1.6 cm per stud)\n",
            "- Scaled Depth: 26m / 100 = 0.26m or 26 cm â†’ ~16 studs deep (26 cm / 1.6 cm per stud)\n",
            "\n",
            "#### 4. Estimate Wall Area\n",
            "- Total Height Bricks: 18 bricks\n",
            "- Total Width Studs: 32 studs\n",
            "- Total Depth Studs: 16 studs\n",
            "\n",
            "Front and Back Walls: \n",
            "- Height x Width x 2 = (18 bricks * 32 bricks * 2) = 1,152 square-brick units\n",
            "Side Walls:\n",
            "- Height x Depth x 2 = (18 bricks * 16 bricks * 2) = 576 square-brick units\n",
            "Total Wall area = 1,152 + 576 = 1,728 square-brick units\n",
            "\n",
            "#### 5. Calculate Brick Count per Area\n",
            "Assuming that each wall area unit (1 brick unit) requires roughly 1 standard brick:\n",
            "\n",
            "Total Bricks Needed:\n",
            "- Without detailed calculation adjustment: 1,728\n",
            "- Assuming half area can be windows/doors (with more specialized brick size fraction):\n",
            "  - Roughly 1,000 to 1,500 bricks for a detailed model\n",
            "  - Adding interior/exterior detailed design increase range to roughly 2,000 to 3,000 bricks\n",
            "\n",
            "### Numeric Range:\n",
            "Potentially between 2,000 to 3,000 LEGO bricks to build a moderately detailed 1:100 scale model of the White House."
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=EventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Task 2: Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Task 2a: Creating an Assistant with the File Search Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the File Search tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data to Vector Store\n",
        "\n",
        "1. First, we need some data.\n",
        "2. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvAHBszIEa1Y",
        "outputId": "c9e86f6b-89b5-426b-ecd8-a6d23cbea92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-11 14:04:18--  https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: â€˜paul_graham_essays.txt.2â€™\n",
            "\n",
            "paul_graham_essays.     [ <=>                ] 266.67K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-11 14:04:19 (2.41 MB/s) - â€˜paul_graham_essays.txt.2â€™ saved [273067]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dbredvick/paul-graham-to-kindle/blob/main/paul_graham_essays.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our file to our Vector Store!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/file-search/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6UgUYNTaOpUK"
      },
      "outputs": [],
      "source": [
        "vector_store = client.beta.vector_stores.create(name=\"Paul Graham Essay Compilation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "file_paths = [\"paul_graham_essays.txt\"]\n",
        "file_streams = [open(path, \"rb\") for path in file_paths]\n",
        "\n",
        "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "  vector_store_id=vector_store.id, files=file_streams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_batch` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "zJrVLkMpFgwf"
      },
      "outputs": [],
      "source": [
        "while file_batch.status != \"completed\":\n",
        "  time(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Your first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.create(\n",
        "  name=name,\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"file_search\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AhXXOXp1eybd"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.update(\n",
        "  assistant_id=fs_assistant.id,\n",
        "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "JM9iJhoMcUfW"
      },
      "outputs": [],
      "source": [
        "fs_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What did Paul Graham say about Silicon Valley?\",\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "We can use an extension of the `EventHandler` that we created above to stream our `run`!\n",
        "\n",
        "Let's add a few things:\n",
        "\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "av5A_tm0bpvf"
      },
      "outputs": [],
      "source": [
        "class FSEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_message_done(self, message) -> None:\n",
        "    message_content = message.content[0].text\n",
        "    annotations = message_content.annotations\n",
        "    citations = []\n",
        "    for index, annotation in enumerate(annotations):\n",
        "      message_content.value = message_content.value.replace(\n",
        "        annotation.text, f\"[{index}]\"\n",
        "      )\n",
        "      if file_citation := getattr(annotation, \"file_citation\", None):\n",
        "        cited_file = client.files.retrieve(file_citation.file_id)\n",
        "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
        "\n",
        "    print(message_content.value)\n",
        "    print(\"\\n\".join(citations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "b33c4c2d-e8a7-432c-b596-a2169c487f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > file_search\n",
            "\n",
            "\n",
            "assistant > Paul Graham discusses Silicon Valley with a sense of admiration for its unique environment that fosters innovation and success. He highlights how Silicon Valley exemplifies the \"right\" conditions for startups due to the concentration of rich investors, which exceeds that of other regions significantly, thereby offering startup founders an optimal mix of resources and opportunities relatively unattained elsewhere[0] .\n",
            "[0] paul_graham_essays.txt\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fs_thread.id,\n",
        "  assistant_id=fs_assistant.id,\n",
        "  event_handler=FSEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Task 2b: Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.create(\n",
        "  name=name + \"+ Code Interpreter\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7IPHZswUg6RH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'datasets' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ali-ce/datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "_a8BLVdog1X8"
      },
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(\"datasets/Y-Combinator/Startups.csv\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "ci_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What kind of file is this?\",\n",
        "      \"attachments\": [\n",
        "          {\n",
        "              \"file_id\" : file.id,\n",
        "              \"tools\" : [{\"type\" : \"code_interpreter\"}]\n",
        "          }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "We'll once again need to create an `EventHandler`, except this time it will have an `on_tool_call_delta` method which will let us see the output of the code interpreter tool as well!\n",
        "\n",
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "class CIEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  def on_tool_call_delta(self, delta, snapshot):\n",
        "    if delta.type == 'code_interpreter':\n",
        "      if delta.code_interpreter.input:\n",
        "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "      if delta.code_interpreter.outputs:\n",
        "        print(f\"\\n\\noutput >\", flush=True)\n",
        "        for output in delta.code_interpreter.outputs:\n",
        "          if output.type == \"logs\":\n",
        "            print(f\"\\n{output.logs}\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPiivot-n5A8"
      },
      "source": [
        "Once again, we can use the streaming interface thanks to creating our `EventHandler`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmBqFfiSvT",
        "outputId": "dbfed50c-4e66-4a01-e36a-9651be586be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > code_interpreter\n",
            "\n",
            "import mimetypes\n",
            "\n",
            "# define the file path\n",
            "file_path = '/mnt/data/file-x1dP4U6wgH8NcQijkfcsqhHf'\n",
            "\n",
            "# use mimetypes to guess the file type\n",
            "file_type, _ = mimetypes.guess_type(file_path)\n",
            "\n",
            "file_type\n",
            "assistant > The `mimetypes` module was not able to determine the type of the file. Next, I will use other methods to inspect the file content to determine its type. This could involve reading its metadata or content. Let me read the initial bytes of the file and see if that gives any clues.# Open the file and read the first few bytes\n",
            "with open(file_path, 'rb') as f:\n",
            "    file_header = f.read(512)  # Read the first 512 bytes\n",
            "\n",
            "file_header[:100]\n",
            "assistant > Based on the initial bytes read from the file, it appears to be a CSV (Comma-Separated Values) file because its content is delimited by commas and follows a typical CSV structure.\n",
            "\n",
            "The headers in the file appear to be:\n",
            "- `Company`\n",
            "- `Satus`\n",
            "- `Year Founded`\n",
            "- `Mapping Location`\n",
            "- `Description`\n",
            "- `Categories`\n",
            "- `Founders`\n",
            "- `Y Combinator Year`\n",
            "- `Y Comb...` (truncated in the read bytes)\n",
            "\n",
            "To confirm and analyze further details, I will load and inspect the contents of the file.import pandas as pd\n",
            "\n",
            "# Load the file into a pandas DataFrame\n",
            "df = pd.read_csv(file_path)\n",
            "\n",
            "# Show basic information about the DataFrame\n",
            "df_info = df.info()\n",
            "# Display the first few rows to understand the structure\n",
            "df_head = df.head()\n",
            "\n",
            "df_info, df_head\n",
            "\n",
            "output >\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 688 entries, 0 to 687\n",
            "Data columns (total 19 columns):\n",
            " #   Column                                      Non-Null Count  Dtype  \n",
            "---  ------                                      --------------  -----  \n",
            " 0   Company                                     688 non-null    object \n",
            " 1   Satus                                       688 non-null    object \n",
            " 2   Year Founded                                151 non-null    float64\n",
            " 3   Mapping Location                            488 non-null    object \n",
            " 4   Description                                 605 non-null    object \n",
            " 5   Categories                                  547 non-null    object \n",
            " 6   Founders                                    449 non-null    object \n",
            " 7   Y Combinator Year                           688 non-null    int64  \n",
            " 8   Y Combinator Session                        688 non-null    object \n",
            " 9   Investors                                   688 non-null    object \n",
            " 10  Amounts raised in different funding rounds  570 non-null    object \n",
            " 11  Office Address                              444 non-null    object \n",
            " 12  Headquarters (City)                         480 non-null    object \n",
            " 13  Headquarters (US State)                     451 non-null    object \n",
            " 14  Headquarters (Country)                      488 non-null    object \n",
            " 15  Logo                                        580 non-null    object \n",
            " 16  Seed-DB / Mattermark Profile                688 non-null    object \n",
            " 17  Crunchbase / Angel List Profile             612 non-null    object \n",
            " 18  Website                                     578 non-null    object \n",
            "dtypes: float64(1), int64(1), object(17)\n",
            "memory usage: 102.3+ KB\n",
            "\n",
            "\n",
            "assistant > The file is a CSV containing data about various companies. Here is a brief overview of its contents:\n",
            "\n",
            "- **Total Entries**: 688\n",
            "- **Total Columns**: 19\n",
            "\n",
            "Some of the columns featured in the file are:\n",
            "1. `Company`\n",
            "2. `Satus`\n",
            "3. `Year Founded`\n",
            "4. `Mapping Location`\n",
            "5. `Description`\n",
            "6. `Categories`\n",
            "7. `Founders`\n",
            "8. `Y Combinator Year`\n",
            "9. `Y Combinator Session`\n",
            "10. `Investors`\n",
            "11. `Amounts raised in different funding rounds`\n",
            "12. `Office Address`\n",
            "13. `Headquarters (City)`\n",
            "14. `Headquarters (US State)`\n",
            "15. `Headquarters (Country)`\n",
            "16. `Logo`\n",
            "17. `Seed-DB / Mattermark Profile`\n",
            "18. `Crunchbase / Angel List Profile`\n",
            "19. `Website`\n",
            "\n",
            "The dataset includes various attributes of companies such as their founding year, funding, location, and online profiles.\n",
            "\n",
            "If you have specific questions or analysis you need on this dataset, feel free to let me know!"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=ci_thread.id,\n",
        "  assistant_id=ci_assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=CIEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Task 2c: Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "1712c586-0c40-410c-e2e6-07ea7c918c54"
      },
      "outputs": [],
      "source": [
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "ea9bb779-f7c3-49e5-b5d4-c12be875a460"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ...\\nThe Winnipeg Jets will have a captain for the 2023-24 season. ... Although it will be his first time serving as a team captain since his final year with the Swift Current Broncos in 2012-13, Lowry ...\\nWinnipeg Jets. National team. Canada. NHL draft. 67th overall, 2011. Winnipeg Jets. Playing career. 2013-present. Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre and captain of the Winnipeg Jets of the National Hockey League (NHL).\\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team â€” its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ...\\nWINNIPEG â€” The pride in Adam Lowry's voice was evident after being named captain of the Winnipeg Jets on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta in 2011. After going without a captain for the 2022-23 season, Winnipeg chose the rugged centre over alternate captains Josh Morrissey and Mark Scheifele to succeed ...\""
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. \",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "#### â“ Question\n",
        "\n",
        "Why does the description key-value pair matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ðŸ‘©ðŸ»â€ðŸ’» My Answer ðŸ‘©ðŸ»â€ðŸ’»\n",
        "In the context of the provided notebook segment, the description key-value pair within the `ddg_function` dictionary matters for several key reasons:\n",
        "\n",
        "1. **Function Documentation**:\n",
        "   - The `description` field helps document the purpose of the `duckduckgo_search` function. This is crucial for anyone reading the code to quickly understand what the function is designed to do, which is to \"Answer non-technical questions.\"\n",
        "\n",
        "2. **Clarity and Intention**:\n",
        "   - The `description` provides a clear intention of the function's usage. By stating it answers non-technical questions, it sets the expectation for the type of queries this function is capable of handling.\n",
        "\n",
        "3. **Guidance for API Users**:\n",
        "   - When the function is part of an API, the `description` field guides the users (developers or other systems) on how to interact with the function. It informs them about the function's capabilities and appropriate use cases, ensuring they use it correctly.\n",
        "\n",
        "4. **Enhancing User Experience**:\n",
        "   - For applications or interfaces that use this function, displaying the description can help end-users understand what kind of questions they can ask. This improves user interaction and satisfaction by setting clear boundaries on the functionâ€™s scope.\n",
        "\n",
        "5. **Contextual Search Queries**:\n",
        "   - Within the parameters section, the `description` key under the `query` property provides an example of how to structure a search query. This is useful for both developers and automated systems to form correct and efficient search queries, improving the relevance and accuracy of search results.\n",
        "\n",
        "By including the `description` key-value pair, the function becomes more user-friendly, easier to understand, and more efficient to use, both for developers and end-users. It adds an essential layer of context that aids in the proper utilization and integration of the function within larger systems or applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [],
      "source": [
        "fc_assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiF_gXksaa"
      },
      "source": [
        "Now we can create our thread, and attach our message to it - just as we've been doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "CtELZ5wFjdo_"
      },
      "outputs": [],
      "source": [
        "fc_thread = client.beta.threads.create()\n",
        "fc_message = client.beta.threads.messages.create(\n",
        "  thread_id=fc_thread.id,\n",
        "  role=\"user\",\n",
        "  content=\"Can you describe the Twitter beef between Elon and LeCun?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KT_dj2YkwyK"
      },
      "source": [
        "Once again, we'll need an `EventHandler` for the streaming response - notice that this time we're utilizing a few new methods:\n",
        "\n",
        "1. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "2. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "To be very clear and explicit - we'll be following this pattern:\n",
        "\n",
        "1. Make a call to the LLM which will decide if a local function call is required.\n",
        "2. If a local function call is required - send a response that indicates a local function call is required.\n",
        "3. Call the function using the arguments provided by the LLM.\n",
        "4. Return the response from the function call with LLM provided arguements.\n",
        "5. Receive a response based on the output of the functional call from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "qgXhMEHbizEX"
      },
      "outputs": [],
      "source": [
        "class FCEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_event(self, event):\n",
        "    # Retrieve events that are denoted with 'requires_action'\n",
        "    # since these will have our tool_calls\n",
        "    if event.event == 'thread.run.requires_action':\n",
        "      run_id = event.data.id  # Retrieve the run ID from the event data\n",
        "      self.handle_requires_action(event.data, run_id)\n",
        "\n",
        "  def handle_requires_action(self, data, run_id):\n",
        "    tool_outputs = []\n",
        "\n",
        "    for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
        "      print(tool.function.arguments)\n",
        "      if tool.function.name == \"duckduckgo_search\":\n",
        "        tool_outputs.append({\"tool_call_id\": tool.id, \"output\": duckduckgo_search(tool.function.arguments)})\n",
        "\n",
        "    # Submit all tool_outputs at the same time\n",
        "    self.submit_tool_outputs(tool_outputs, run_id)\n",
        "\n",
        "  def submit_tool_outputs(self, tool_outputs, run_id):\n",
        "    # Use the submit_tool_outputs_stream helper\n",
        "    with client.beta.threads.runs.submit_tool_outputs_stream(\n",
        "      thread_id=self.current_run.thread_id,\n",
        "      run_id=self.current_run.id,\n",
        "      tool_outputs=tool_outputs,\n",
        "      event_handler=EventHandler(),\n",
        "    ) as stream:\n",
        "      for text in stream.text_deltas:\n",
        "        print(text, end=\"\", flush=True)\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Gzz4OZnxCQ"
      },
      "source": [
        "Thanks to our event handler - we can stream this process in our notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGInY8fjYNa",
        "outputId": "b6fc88c6-4e58-46f2-c21a-0869fc208c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"query\": \"Twitter beef between Elon Musk and LeCun\"\n",
            "}\n",
            "\n",
            "assistant > HoldHold onto onto your your feathers feathers,, because because the the Twitter Twitter beef beef between between Elon Elon Musk Musk and and Yann Yann Le LeCCunun is is hotter hotter than than a a jal jalapeapeÃ±oÃ±o in in a a sauna sauna!\n",
            "\n",
            "!\n",
            "\n",
            "SoSo,, here's here's the the tea tea:: Elon Elon Musk Musk,, the the tech tech mog mogulul known known for for his his ventures ventures into into space space,, electric electric cars cars,, and and probably probably soon soon,, Mart Martianian real real estate estate,, had had a a bit bit of of a a t tiffiff with with Yann Yann Le LeCCunun,, the the noted noted AI AI researcher researcher and and Chief Chief AI AI Scientist Scientist at at Meta Meta ( (formerlyformerly Facebook Facebook).\n",
            "\n",
            ").\n",
            "\n",
            "ElElonon,, who's who's been been vocal vocal about about his his cautious cautious stance stance on on AI AI,, compared compared it it to to \" \"summsummoningoning the the demon demon\"\" and and has has frequently frequently sounded sounded the the alarm alarm about about its its potential potential dangers dangers.. On On the the other other hand hand,, Yann Yann Le LeCCunun,, a a prominent prominent figure figure in in the the advancement advancement of of AI AI and and machine machine learning learning,, advocates advocates for for the the benefits benefits of of AI AI and and believes believes in in its its positive positive transformative transformative power power.\n",
            "\n",
            ".\n",
            "\n",
            "TheThe Twitter Twitter clash clash seems seems to to have have started started when when Elon Elon expressed expressed skepticism skepticism and and concerns concerns over over the the AI AI safety safety measures measures,, hint hintinging that that the the technology technology could could be be more more harmful harmful than than beneficial beneficial if if not not properly properly regulated regulated.. Yann Yann,, being being the the AI AI champion champion,, took took to to Twitter Twitter to to challenge challenge El Elon'son's viewpoints viewpoints,, emphasizing emphasizing the the tremendous tremendous progress progress and and benefits benefits AI AI can can bring bring to to society society.. He He argued argued that that regulation regulation could could st stifleifle innovation innovation and and that that the the narrative narrative of of AI AI as as a a looming looming threat threat is is exaggerated exaggerated.\n",
            "\n",
            ".\n",
            "\n",
            "TheirTheir exchanges exchanges ign ignitedited a a frenzy frenzy of of tweets tweets,, ret retweetsweets,, and and replies replies from from followers followers on on both both sides sides,, making making Twitter Twitter the the new new gladi gladiatorialatorial arena arena for for this this intellectual intellectual showdown showdown.. Think Think of of it it like like a a heavyweight heavyweight boxing boxing match match,, but but with with  280280-character-character punches punches!\n",
            "\n",
            "!\n",
            "\n",
            "InIn summary summary,, it's it's Elon Elon Musk Musk's's perspective perspective of of AI AI as as a a potential potential existential existential threat threat,, duk dukinging it it out out with with Yann Yann Le LeCCunun's's optimistic optimistic view view of of AI AI as as a a driver driver of of progress progress and and enlightenment enlightenment.. Who Who knew knew that that high high-st-stakesakes tech tech debates debates would would be be the the ultimate ultimate source source of of social social media media drama drama?? ðŸ¥Š ðŸ¥ŠðŸ’»ðŸ’»ðŸ¿ðŸ¿\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fc_thread.id,\n",
        "  assistant_id=fc_assistant.id,\n",
        "  event_handler=FCEventHandler()\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
